{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Video detection using traind model"
      ],
      "metadata": {
        "id": "CnLxnCBEZ4GK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can use the trained model to detect a video. \n",
        "\n",
        "The priciple is to separate the video into hundureds of frames (images), and detect images one by one, then put those detected image togethor to form a detected video. "
      ],
      "metadata": {
        "id": "x-BM4rVfZ7KT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Table of Content\n",
        "* Set up env\n",
        "* Load the trained model\n",
        "* Separate video into frames and run detection on them\n",
        "* Make new video using detected frames"
      ],
      "metadata": {
        "id": "MmIktqOaaAhO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Set up env"
      ],
      "metadata": {
        "id": "1A_73-lkaJNS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Path and version numbers for current python\n",
        "!which python\n",
        "!python --version"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n69uHWMrSIBl",
        "outputId": "46b82664-9eeb-462f-c82a-31f43b0bd58e"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/bin/python\n",
            "Python 3.8.16\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Check the tensorflow and keras version\n",
        "import tensorflow as tf\n",
        "import keras\n",
        "print(keras.__version__)\n",
        "print(tf.__version__)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M06AXVa4yAfO",
        "outputId": "06f7f98b-42b4-4415-b362-bf53370dde97"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.9.0\n",
            "2.9.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# clone Mask_RCNN repo and install packages\n",
        "\n",
        "%%shell\n",
        "git clone https://github.com/akTwelve/Mask_RCNN.git\n",
        "cd Mask_RCNN\n",
        "python setup.py install"
      ],
      "metadata": {
        "id": "JmhZtqlrpTw8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bae83c29-de87-46e6-c63a-0b2f465ca6b2"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'Mask_RCNN'...\n",
            "remote: Enumerating objects: 979, done.\u001b[K\n",
            "remote: Total 979 (delta 0), reused 0 (delta 0), pack-reused 979\u001b[K\n",
            "Receiving objects: 100% (979/979), 137.72 MiB | 27.54 MiB/s, done.\n",
            "Resolving deltas: 100% (570/570), done.\n",
            "WARNING:root:Fail load requirements file, so using default ones.\n",
            "/usr/local/lib/python3.8/dist-packages/setuptools/dist.py:697: UserWarning: Usage of dash-separated 'description-file' will not be supported in future versions. Please use the underscore name 'description_file' instead\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/setuptools/dist.py:697: UserWarning: Usage of dash-separated 'license-file' will not be supported in future versions. Please use the underscore name 'license_file' instead\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/setuptools/dist.py:697: UserWarning: Usage of dash-separated 'requirements-file' will not be supported in future versions. Please use the underscore name 'requirements_file' instead\n",
            "  warnings.warn(\n",
            "running install\n",
            "running bdist_egg\n",
            "running egg_info\n",
            "creating mask_rcnn.egg-info\n",
            "writing mask_rcnn.egg-info/PKG-INFO\n",
            "writing dependency_links to mask_rcnn.egg-info/dependency_links.txt\n",
            "writing top-level names to mask_rcnn.egg-info/top_level.txt\n",
            "writing manifest file 'mask_rcnn.egg-info/SOURCES.txt'\n",
            "reading manifest template 'MANIFEST.in'\n",
            "adding license file 'LICENSE'\n",
            "writing manifest file 'mask_rcnn.egg-info/SOURCES.txt'\n",
            "installing library code to build/bdist.linux-x86_64/egg\n",
            "running install_lib\n",
            "running build_py\n",
            "creating build\n",
            "creating build/lib\n",
            "creating build/lib/mrcnn\n",
            "copying mrcnn/visualize.py -> build/lib/mrcnn\n",
            "copying mrcnn/model.py -> build/lib/mrcnn\n",
            "copying mrcnn/parallel_model.py -> build/lib/mrcnn\n",
            "copying mrcnn/utils.py -> build/lib/mrcnn\n",
            "copying mrcnn/__init__.py -> build/lib/mrcnn\n",
            "copying mrcnn/config.py -> build/lib/mrcnn\n",
            "creating build/bdist.linux-x86_64\n",
            "creating build/bdist.linux-x86_64/egg\n",
            "creating build/bdist.linux-x86_64/egg/mrcnn\n",
            "copying build/lib/mrcnn/visualize.py -> build/bdist.linux-x86_64/egg/mrcnn\n",
            "copying build/lib/mrcnn/model.py -> build/bdist.linux-x86_64/egg/mrcnn\n",
            "copying build/lib/mrcnn/parallel_model.py -> build/bdist.linux-x86_64/egg/mrcnn\n",
            "copying build/lib/mrcnn/utils.py -> build/bdist.linux-x86_64/egg/mrcnn\n",
            "copying build/lib/mrcnn/__init__.py -> build/bdist.linux-x86_64/egg/mrcnn\n",
            "copying build/lib/mrcnn/config.py -> build/bdist.linux-x86_64/egg/mrcnn\n",
            "byte-compiling build/bdist.linux-x86_64/egg/mrcnn/visualize.py to visualize.cpython-38.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/mrcnn/model.py to model.cpython-38.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/mrcnn/parallel_model.py to parallel_model.cpython-38.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/mrcnn/utils.py to utils.cpython-38.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/mrcnn/__init__.py to __init__.cpython-38.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/mrcnn/config.py to config.cpython-38.pyc\n",
            "creating build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying mask_rcnn.egg-info/PKG-INFO -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying mask_rcnn.egg-info/SOURCES.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying mask_rcnn.egg-info/dependency_links.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying mask_rcnn.egg-info/top_level.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "zip_safe flag not set; analyzing archive contents...\n",
            "creating dist\n",
            "creating 'dist/mask_rcnn-2.1-py3.8.egg' and adding 'build/bdist.linux-x86_64/egg' to it\n",
            "removing 'build/bdist.linux-x86_64/egg' (and everything under it)\n",
            "Processing mask_rcnn-2.1-py3.8.egg\n",
            "Copying mask_rcnn-2.1-py3.8.egg to /usr/local/lib/python3.8/dist-packages\n",
            "Adding mask-rcnn 2.1 to easy-install.pth file\n",
            "\n",
            "Installed /usr/local/lib/python3.8/dist-packages/mask_rcnn-2.1-py3.8.egg\n",
            "Processing dependencies for mask-rcnn==2.1\n",
            "Finished processing dependencies for mask-rcnn==2.1\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import sys\n",
        "import random\n",
        "import math\n",
        "import numpy as np\n",
        "import skimage.io\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Root directory of the project\n",
        "ROOT_DIR = os.path.abspath(\"./Mask_RCNN/\")\n",
        "\n",
        "# Import Mask RCNN\n",
        "sys.path.append(ROOT_DIR)  # To find local version of the library\n",
        "from mrcnn import utils\n",
        "import mrcnn.model as modellib\n",
        "from mrcnn import visualize\n",
        "\n",
        "# Import COCO config\n",
        "sys.path.append(os.path.join(ROOT_DIR, \"samples/coco/\"))  # find local version\n",
        "import coco\n",
        "\n",
        "%matplotlib inline \n",
        "\n",
        "# Directory to save logs and trained model\n",
        "MODEL_DIR = os.path.join(ROOT_DIR, \"logs\")\n",
        "\n",
        "# Local path to trained weights file\n",
        "COCO_MODEL_PATH = os.path.join(ROOT_DIR, \"mask_rcnn_coco.h5\")\n",
        "# Download COCO trained weights from Releases if needed\n",
        "if not os.path.exists(COCO_MODEL_PATH):\n",
        "    utils.download_trained_weights(COCO_MODEL_PATH)\n",
        "\n",
        "# Directory of images to run detection on\n",
        "IMAGE_DIR = os.path.join(ROOT_DIR, \"images\")"
      ],
      "metadata": {
        "id": "bf6iBxTJaPoH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "23f77d4f-942d-4391-e2c1-e98707d8abe8"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading pretrained model to /content/Mask_RCNN/mask_rcnn_coco.h5 ...\n",
            "... done downloading pretrained model!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# download original video and model from my github\n",
        "!git lfs clone https://github.com/BaosenZ/amoeba-video-detection.git"
      ],
      "metadata": {
        "id": "rdIHna1VaROD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "de502056-bbc0-4961-8bee-ca97ab43afa5"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING: 'git lfs clone' is deprecated and will not be updated\n",
            "          with new flags from 'git clone'\n",
            "\n",
            "'git clone' has been updated in upstream Git to have comparable\n",
            "speeds to 'git lfs clone'.\n",
            "Cloning into 'amoeba-video-detection'...\n",
            "remote: Enumerating objects: 250, done.\u001b[K\n",
            "remote: Counting objects: 100% (39/39), done.\u001b[K\n",
            "remote: Compressing objects: 100% (30/30), done.\u001b[K\n",
            "remote: Total 250 (delta 7), reused 39 (delta 7), pack-reused 211\u001b[K\n",
            "Receiving objects: 100% (250/250), 142.83 MiB | 38.87 MiB/s, done.\n",
            "Resolving deltas: 100% (110/110), done.\n",
            "Git LFS: (1 of 1 files) 225.96 MB / 225.96 MB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !rm -rf amoeba-video-detection"
      ],
      "metadata": {
        "id": "5FGX7T-lo9YJ"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip amoeba-video-detection/trained-amoeba-model.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hfR9IcN-k0ky",
        "outputId": "6026a198-a5a2-44fa-b16b-978b3f466359"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  amoeba-video-detection/trained-amoeba-model.zip\n",
            "  inflating: trained-amoeba-model/mask_rcnn_amoeba_cfg_0009.h5  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load the trained model"
      ],
      "metadata": {
        "id": "J01YD8dYaWVg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class_names = ['BG', 'amoeba']"
      ],
      "metadata": {
        "id": "DRBO7fnInsTf"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class InferenceConfig(coco.CocoConfig):\n",
        "    NAME = \"amoeba_cfg\"\n",
        "    GPU_COUNT = 1\n",
        "    IMAGES_PER_GPU = 3\n",
        "    batch_size = 3  # Batch size = GPU_COUNT * IMAGES_PER_GPU\n",
        "    NUM_CLASSES = 1 + 1\n",
        "\n",
        "config = InferenceConfig()\n",
        "config.display()"
      ],
      "metadata": {
        "id": "myuyI9BraXJR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fcccff35-a127-4f22-9bff-19c603a8aea7"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Configurations:\n",
            "BACKBONE                       resnet101\n",
            "BACKBONE_STRIDES               [4, 8, 16, 32, 64]\n",
            "BATCH_SIZE                     3\n",
            "BBOX_STD_DEV                   [0.1 0.1 0.2 0.2]\n",
            "COMPUTE_BACKBONE_SHAPE         None\n",
            "DETECTION_MAX_INSTANCES        100\n",
            "DETECTION_MIN_CONFIDENCE       0.7\n",
            "DETECTION_NMS_THRESHOLD        0.3\n",
            "FPN_CLASSIF_FC_LAYERS_SIZE     1024\n",
            "GPU_COUNT                      1\n",
            "GRADIENT_CLIP_NORM             5.0\n",
            "IMAGES_PER_GPU                 3\n",
            "IMAGE_CHANNEL_COUNT            3\n",
            "IMAGE_MAX_DIM                  1024\n",
            "IMAGE_META_SIZE                14\n",
            "IMAGE_MIN_DIM                  800\n",
            "IMAGE_MIN_SCALE                0\n",
            "IMAGE_RESIZE_MODE              square\n",
            "IMAGE_SHAPE                    [1024 1024    3]\n",
            "LEARNING_MOMENTUM              0.9\n",
            "LEARNING_RATE                  0.001\n",
            "LOSS_WEIGHTS                   {'rpn_class_loss': 1.0, 'rpn_bbox_loss': 1.0, 'mrcnn_class_loss': 1.0, 'mrcnn_bbox_loss': 1.0, 'mrcnn_mask_loss': 1.0}\n",
            "MASK_POOL_SIZE                 14\n",
            "MASK_SHAPE                     [28, 28]\n",
            "MAX_GT_INSTANCES               100\n",
            "MEAN_PIXEL                     [123.7 116.8 103.9]\n",
            "MINI_MASK_SHAPE                (56, 56)\n",
            "NAME                           amoeba_cfg\n",
            "NUM_CLASSES                    2\n",
            "POOL_SIZE                      7\n",
            "POST_NMS_ROIS_INFERENCE        1000\n",
            "POST_NMS_ROIS_TRAINING         2000\n",
            "PRE_NMS_LIMIT                  6000\n",
            "ROI_POSITIVE_RATIO             0.33\n",
            "RPN_ANCHOR_RATIOS              [0.5, 1, 2]\n",
            "RPN_ANCHOR_SCALES              (32, 64, 128, 256, 512)\n",
            "RPN_ANCHOR_STRIDE              1\n",
            "RPN_BBOX_STD_DEV               [0.1 0.1 0.2 0.2]\n",
            "RPN_NMS_THRESHOLD              0.7\n",
            "RPN_TRAIN_ANCHORS_PER_IMAGE    256\n",
            "STEPS_PER_EPOCH                1000\n",
            "TOP_DOWN_PYRAMID_SIZE          256\n",
            "TRAIN_BN                       False\n",
            "TRAIN_ROIS_PER_IMAGE           200\n",
            "USE_MINI_MASK                  True\n",
            "USE_RPN_ROIS                   True\n",
            "VALIDATION_STEPS               50\n",
            "WEIGHT_DECAY                   0.0001\n",
            "batch_size                     3\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create model object in inference mode and Load weights.\n",
        "Trained_Model_Path = \"trained-amoeba-model/mask_rcnn_amoeba_cfg_0009.h5\"\n",
        "model = modellib.MaskRCNN(mode=\"inference\", model_dir=MODEL_DIR, config=config)\n",
        "model.load_weights(Trained_Model_Path, by_name=True)"
      ],
      "metadata": {
        "id": "2tWAzbnCaroR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8922a4d9-32e6-4db7-f7ff-ea8fa1c3f14a"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.8/dist-packages/tensorflow/python/util/deprecation.py:629: calling map_fn_v2 (from tensorflow.python.ops.map_fn) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use fn_output_signature instead\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## if run pretrained coco model in Mask RCNN package"
      ],
      "metadata": {
        "id": "TYS9spCab6GH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# COCO dataset class names\n",
        "# class_names = ['BG', 'person', 'bicycle','car', 'motorcycle','airplane',\n",
        "#                   'bus', 'train', 'truck', 'boat', 'traffic light',\n",
        "#                   'fire hydrant', 'stop sign', 'parking meter','bench','bird',\n",
        "#                   'cat','dog','horse','sheep','cow','elephant','bear',\n",
        "#                   'zebra','giraffe','backpack','umbrella','handbag','tie',\n",
        "#                   'suitcase', 'frisbee', 'skis','snowboard','sports ball',\n",
        "#                   'kite','baseball bat','baseball glove', 'skateboard',\n",
        "#                   'surfboard','tennis racket','bottle','wine glass','cup',\n",
        "#                   'fork','knife','spoon','bowl','banana','apple',\n",
        "#                   'sandwich','orange','broccoli','carrot','hot dog', 'pizza',\n",
        "#                   'donut','cake','chair','couch','potted plant','bed',\n",
        "#                   'dining table','toilet','tv','laptop','nouse','remote',\n",
        "#                   'keyboard','cell phone','microwave','oven','toaster',\n",
        "#                   'sink','refrigerator','book','clock','vase','scissors',\n",
        "#                   'teddy bear','hair drier','toothbrush']"
      ],
      "metadata": {
        "id": "VnkyJX5Kno4j"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# class InferenceConfig(coco.CocoConfig):\n",
        "#     GPU_COUNT = 1\n",
        "#     IMAGES_PER_GPU = 3\n",
        "#     batch_size = 3 # Batch size = GPU_COUNT * IMAGES_PER_GPU\n",
        "#     NUM_CLASSES = 80 + 1\n",
        "    \n",
        "# config = InferenceConfig()\n",
        "# config.display()"
      ],
      "metadata": {
        "id": "QlE6yvtXnI4X"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create model object in inference mode and Load weights.\n",
        "# model = modellib.MaskRCNN(mode=\"inference\", model_dir=MODEL_DIR, config=config)\n",
        "# model.load_weights(COCO_MODEL_PATH, by_name=True, exclude=[ \"mrcnn_class_logits\", \"mrcnn_bbox_fc\", \"mrcnn_bbox\", \"mrcnn_mask\"])"
      ],
      "metadata": {
        "id": "H8m0lZJ1b2EG"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Separate video into frames and run detection on them"
      ],
      "metadata": {
        "id": "SNMo6JB4a0Uq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import glob\n",
        "ROOT_DIR = os.getcwd()\n",
        "VIDEO_PATH = os.path.join(ROOT_DIR, \"amoeba-video-detection/original-video/amoeba.mp4\")   # choose video file\n",
        "print(VIDEO_PATH)\n",
        "DETECT_IMAGE_SAVE_DIR = os.path.join(ROOT_DIR, \"output-video-images\")\n",
        "print(DETECT_IMAGE_SAVE_DIR)\n",
        "\n",
        "if not os.path.exists(DETECT_IMAGE_SAVE_DIR):\n",
        "    os.makedirs(DETECT_IMAGE_SAVE_DIR)"
      ],
      "metadata": {
        "id": "1_2uazMya1Ga",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "37557772-4515-4435-d984-2417d6a0d277"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/amoeba-video-detection/original-video/amoeba.mp4\n",
            "/content/output-video-images\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import random\n",
        "# import math\n",
        "# import numpy as np\n",
        "# import scipy.misc\n",
        "\n",
        "# define random colors\n",
        "# def random_colors(N):\n",
        "#     np.random.seed(1)\n",
        "#     colors = [tuple(255 * np.random.rand(3)) for _ in range(N)]\n",
        "#     return colors\n",
        "\n",
        "#apply mask to image\n",
        "# def apply_mask(image, mask, color, alpha=0.5):\n",
        "#     for n, c in enumerate(color):\n",
        "#         image[:,:,n]=np.where(mask==1, image[:,:,n]*(1-alpha)+alpha *c, image[:,:,n])\n",
        "#     return image\n",
        "\n",
        "#take the image and apply the mask, box, and Label\n",
        "# def display_instances(image, boxes, masks, ids, names, scores):\n",
        "#     n_instances = boxes.shape[0]\n",
        "#     colors = random_colors(n_instances)\n",
        "#     if not n_instances:\n",
        "#         print('NO INSTANCES TO DISPLAY')\n",
        "#     else:\n",
        "#         assert boxes.shape[0] == masks.shape[-1] == ids.shape[0]\n",
        "    \n",
        "#     for i, color in enumerate(colors):\n",
        "#          if not np.any(boxes[i]):\n",
        "#             continue\n",
        "    \n",
        "#     y1, x1, y2, x2 = boxes[i]\n",
        "#     label = names[ids[i]]\n",
        "#     score = scores[i] if scores is not None else None\n",
        "#     caption = '{} {:.2f}'.format(label, score) if score else label\n",
        "#     # mask = masks[:, :, i]\n",
        "#     # image = apply_mask(image, mask, color)\n",
        "#     image = cv2.rectangle(image, (x1, y1), (x2, y2), color, 2)\n",
        "#     image = cv2.putText(image, caption, (x1, y1), cv2.FONT_HERSHEY_COMPLEX, 0.7, color, 2)\n",
        "#     return image"
      ],
      "metadata": {
        "id": "JCIq7uAarIKa"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "\n",
        "capture = cv2.VideoCapture(VIDEO_PATH)\n",
        "frames =[]\n",
        "frame_count = 0 \n",
        "batch_size = 3\n",
        "\n",
        "# Find OpenCV version\n",
        "(major_ver, minor_ver, subminor_ver) = (cv2.__version__).split('.')\n",
        "if int(major_ver) < 3 :\n",
        "    fps = capture.get(cv2.cv.CV_CAP_PROP_FPS)\n",
        "    print(\"Frames per second using video.get(cv2.cv.CV_CAP_PROP_FPS): {0}\".format(fps))\n",
        "else :\n",
        "    fps = capture.get(cv2.CAP_PROP_FPS)\n",
        "    print(\"Frames per second using video.get(cv2.CAP_PROP_FPS) : {0}\".format(fps))\n",
        "\n",
        "# Separate video into frames and run detection on them\n",
        "while True:\n",
        "    ret, frame = capture.read()\n",
        "    # Bail out when the video file ends\n",
        "    if not ret:\n",
        "        break        \n",
        "    # Save each frame of the video to a list\n",
        "    frame_count += 1\n",
        "    frames.append(frame)\n",
        "    if len(frames) == batch_size:\n",
        "        results = model.detect(frames, verbose=0)\n",
        "        for i, item in enumerate(zip(frames, results)):\n",
        "            frame = item[0]\n",
        "            r = item[1]\n",
        "            # frame = display_instances(frame, r['rois'], r['masks'], r['class_ids'], class_names, r['scores'])\n",
        "            # frame = apply_mask(frame, r['rois'], color)\n",
        "            for box in r['rois']:\n",
        "                startY, startX, endY, endX = box\n",
        "                frame = cv2.rectangle(frame, (startX, startY), (endX, endY), (255,0,0), 2)\n",
        "            name = '{0}.jpg'.format(frame_count + i - batch_size)\n",
        "            name = os.path.join(DETECT_IMAGE_SAVE_DIR, name)\n",
        "            print(name)\n",
        "            cv2.imwrite(name, frame)\n",
        "        # Clear the frames array to start the next batch\n",
        "        frames = []"
      ],
      "metadata": {
        "id": "p5pjJ2-Da-8m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get all image file paths to a list.\n",
        "images = list(glob.iglob(os.path.join(DETECT_IMAGE_SAVE_DIR, '*.*')))\n",
        "\n",
        "# Sort the images by name index.\n",
        "images = sorted(images, key=lambda x: float(os.path.split(x)[1][:-3]))"
      ],
      "metadata": {
        "id": "yd4w6LlzbDFH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Make new video using detected frames"
      ],
      "metadata": {
        "id": "-TTdw5rCbdBC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def make_video(outvid, images=None, fps=30, size=None,\n",
        "               is_color=True, format=\"FMP4\"):\n",
        "    \"\"\"\n",
        "    Create a video from a list of images.\n",
        " \n",
        "    @param      outvid      output video\n",
        "    @param      images      list of images to use in the video\n",
        "    @param      fps         frame per second\n",
        "    @param      size        size of each frame\n",
        "    @param      is_color    color\n",
        "    @param      format      see http://www.fourcc.org/codecs.php\n",
        "    @return                 see http://opencv-python-tutroals.readthedocs.org/en/latest/py_tutorials/py_gui/py_video_display/py_video_display.html\n",
        "    \"\"\"\n",
        "    from cv2 import VideoWriter, VideoWriter_fourcc, imread, resize\n",
        "    fourcc = VideoWriter_fourcc(*format)\n",
        "    vid = None\n",
        "    for image in images:\n",
        "        if not os.path.exists(image):\n",
        "            raise FileNotFoundError(image)\n",
        "        img = imread(image)\n",
        "        if vid is None:\n",
        "            if size is None:\n",
        "                size = img.shape[1], img.shape[0]\n",
        "            vid = VideoWriter(outvid, fourcc, float(fps), size, is_color)\n",
        "        if size[0] != img.shape[1] and size[1] != img.shape[0]:\n",
        "            img = resize(img, size)\n",
        "        vid.write(img)\n",
        "    vid.release()\n",
        "    return vid\n",
        "\n",
        "outvid = \"output.mp4\"\n",
        "make_video(outvid, images, fps=30)"
      ],
      "metadata": {
        "id": "Nrg5d70DbJp-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from google.colab import files\n",
        "# files.download('output.mp4')"
      ],
      "metadata": {
        "id": "KDVdNcIYbQmm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# References"
      ],
      "metadata": {
        "id": "JIAAPmov1Fra"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Good visualize with openCV: https://pyimagesearch.com/2018/11/19/mask-r-cnn-with-opencv/ and https://zhuanlan.zhihu.com/p/84149055.\n",
        "2. Some codes are modified from the blog: https://www.dlology.com/blog/how-to-run-object-detection-and-segmentation-on-video-fast-for-free/. "
      ],
      "metadata": {
        "id": "AJZ9A0SFEVjC"
      }
    }
  ]
}